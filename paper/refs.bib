@inproceedings{dai2022knowledgeneurons,
  title     = {Knowledge Neurons in Pretrained Transformers},
  author    = {Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2022},
  doi       = {10.18653/v1/2022.acl-long.581},
  url       = {https://arxiv.org/abs/2104.08696}
}

@inproceedings{meng2022rome,
  title     = {Locating and Editing Factual Associations in {GPT}},
  author    = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.05262}
}

@inproceedings{meng2023memit,
  title     = {Mass-Editing Memory in a Transformer},
  author    = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2023},
  url       = {https://arxiv.org/abs/2210.07229}
}

@inproceedings{geva2021kv,
  title     = {Transformer Feed-Forward Layers Are Key-Value Memories},
  author    = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2021},
  doi       = {10.18653/v1/2021.emnlp-main.446},
  url       = {https://arxiv.org/abs/2012.14913}
}

@misc{qwen25,
  title        = {{Qwen2.5-7B} Model Card},
  author       = {{Qwen Team}},
  howpublished = {Hugging Face},
  year         = {2024},
  note         = {Accessed 2026-02-10},
  url          = {https://huggingface.co/Qwen/Qwen2.5-7B}
}

@misc{popqa,
  title        = {PopQA Dataset Card},
  author       = {{PopQA Authors}},
  howpublished = {Hugging Face Datasets},
  year         = {2023},
  note         = {Accessed 2026-02-10},
  url          = {https://huggingface.co/datasets/akariasai/PopQA}
}
